{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e2d71e5-728e-4831-8f08-bc790f765dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import pickle\n",
    "from torchmetrics.regression import PearsonCorrCoef\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "860aac3a-d4d7-4b56-8cb6-db349f194ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abs_error_histograms  MatchBoxNet\t\t\t__pycache__\n",
      "augmentation.py       model_eval_on_given_dataset.py\ttrain.py\n",
      "config.py\t      model_eval.py\t\t\tUntitled.ipynb\n",
      "data_setup.py\t      model_eval_res_common_voices.csv\tutils.py\n",
      "inference.py\t      model.py\n",
      "LSTM\t\t      models\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "beecc0ab-9dca-46b4-a5ed-b6ea348e8ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Model                              Corpus  #audios  \\\n",
      "0  rMatchBoxNet-3x2x112              LibriSpeech-test-clean     2620   \n",
      "1  rMatchBoxNet-3x2x112                        Common Voice    14813   \n",
      "2  rMatchBoxNet-3x2x112                        Common Voice   163387   \n",
      "3  rMatchBoxNet-3x2x112                        Common Voice     6717   \n",
      "4  rMatchBoxNet-3x2x112                        Common Voice   236600   \n",
      "5  rMatchBoxNet-3x2x112                        Common Voice   292303   \n",
      "6  rMatchBoxNet-3x2x112  LibriSpeech test-clean-sil-removed     2620   \n",
      "7  rMatchBoxNet-3x2x112  LibriSpeech test-clean-sil-removed     2620   \n",
      "\n",
      "   Language  MSE_csyl  PCC_csyl  MSE_sp_rate  PCC_sp_rate  \n",
      "0   English   16.8507    0.9918       0.3168       0.8632  \n",
      "1  Armenian   18.4633    0.9399       0.5852       0.8106  \n",
      "2   Russian   10.7650    0.9593       0.4041       0.8721  \n",
      "3   English    6.5434    0.9233       0.2339       0.8248  \n",
      "4   Italian  104.0783    0.6590      13.2657       0.2781  \n",
      "5   Spanish   12.5045    0.9291       0.5307       0.8566  \n",
      "6   English   18.1732    0.9920       0.4815       0.8018  \n",
      "7   English   18.1732    0.9920       0.4815       0.8018  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'model_eval_res_common_voices.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3537918-c3e5-47d3-b0d7-2a8e5eccc9ba",
   "metadata": {},
   "source": [
    "How many syllables will the model say incorrect depends on the length of the audios, the distribution of audio lengthes of the corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
